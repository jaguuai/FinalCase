import os
import re
import pandas as pd
from dotenv import load_dotenv
from neo4j import GraphDatabase
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.neo4j_vector import Neo4jVector
from langchain.schema import Document
from typing import List, Dict, Any
import traceback

# .env dosyasÄ±nÄ± yÃ¼kle
# Bu yolun projenizdeki .env dosyasÄ±nÄ±n gerÃ§ek konumunu gÃ¶sterdiÄŸinden emin olun.
load_dotenv("C:/Users/alice/OneDrive/MasaÃ¼stÃ¼/P2E-Economy-Assistant-AuroryGame/neo4j.env")

class VectorEmbeddingPipeline:
    def __init__(self):
        """
        Vector embedding pipeline with OpenAI using .env variables
        """
        # Neo4j bilgilerini .env'den al
        self.neo4j_uri = os.getenv("NEO4J_URI")
        self.neo4j_user = os.getenv("NEO4J_USERNAME")
        self.neo4j_password = os.getenv("NEO4J_PASSWORD")
        openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # Driver'Ä± iliÅŸkiler iÃ§in ayrÄ± tut
        # Bu sÃ¼rÃ¼cÃ¼, manuel Cypher sorgularÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak ve iliÅŸkiler oluÅŸturmak iÃ§in kullanÄ±lÄ±r.
        self.driver = GraphDatabase.driver(
            self.neo4j_uri, 
            auth=(self.neo4j_user, self.neo4j_password)
        )
        
        # Metinleri vektÃ¶r temsillerine dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in embedding modelini baÅŸlat
        # NOT: LangChain 0.0.9'dan sonra OpenAIEmbeddings sÄ±nÄ±fÄ± 'langchain-openai' paketine taÅŸÄ±nmÄ±ÅŸtÄ±r.
        # Bu paketi 'pip install -U langchain-openai' ile kurmanÄ±z ve
        # 'from langchain_openai import OpenAIEmbeddings' olarak import etmeniz Ã¶nerilir.
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=openai_api_key
        )
        
        # BÃ¼yÃ¼k metinleri yÃ¶netilebilir parÃ§alara bÃ¶lmek iÃ§in metin bÃ¶lÃ¼cÃ¼yÃ¼ yapÄ±landÄ±r
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,     # Her metin parÃ§asÄ±nÄ±n maksimum boyutu
            chunk_overlap=200,   # ParÃ§alar arasÄ±ndaki Ã§akÄ±ÅŸma miktarÄ± (baÄŸlamÄ± korumak iÃ§in)
            length_function=len, # Uzunluk hesaplama fonksiyonu (karakter sayÄ±sÄ±)
        )

    def test_connection(self) -> bool:
        """Neo4j baÄŸlantÄ±sÄ±nÄ± test eder."""
        try:
            with self.driver.session() as session:
                result = session.run("RETURN 'BaÄŸlantÄ± baÅŸarÄ±lÄ±!' AS message")
                print("âœ… Neo4j baÄŸlantÄ± testi:", result.single()["message"])
            return True
        except Exception as e:
            print(f"âŒ Neo4j baÄŸlantÄ± hatasÄ±: {str(e)}")
            traceback.print_exc() # Hata izini yazdÄ±r (hata ayÄ±klama iÃ§in faydalÄ±)
            return False

    def process_pdf_documents(self, pdf_folder_path: str) -> List[Document]:
        """
        Belirtilen klasÃ¶rdeki PDF dosyalarÄ±nÄ± iÅŸler ve LangChain Document nesneleri listesi dÃ¶ndÃ¼rÃ¼r.
        DAO Ã¶nerileri iÃ§in proposal_id'yi dosya adÄ±ndan Ã§Ä±karmaya Ã§alÄ±ÅŸÄ±r.
        """
        documents = []
        
        for filename in os.listdir(pdf_folder_path):
            if filename.endswith('.pdf'):
                file_path = os.path.join(pdf_folder_path, filename)
                try:
                    proposal_id = None
                    # Dosya adÄ±ndan proposal_id Ã§Ä±karmak iÃ§in regex kullanÄ±lÄ±r.
                    # Ã–rnek: "DAOry Proposal #7.docx.pdf" -> "7"
                    match = re.search(r'#(\d+)', filename) 
                    if match:
                        proposal_id = match.group(1) 
                    elif 'Proposal' in filename: 
                        match = re.search(r'Proposal.*?(\d+)', filename, re.IGNORECASE)
                        if match:
                            proposal_id = match.group(1)
                    
                    loader = PyPDFLoader(file_path)
                    pdf_pages = loader.load() # PDF'i sayfalara yÃ¼kler
                    
                    for page in pdf_pages:
                        # Her sayfa iÃ§eriÄŸini parÃ§alara ayÄ±rÄ±r
                        chunks = self.text_splitter.split_text(page.page_content)
                        
                        for i, chunk in enumerate(chunks):
                            # Her parÃ§a iÃ§in meta verileri oluÅŸturur
                            metadata = {
                                "source": filename,
                                "page": page.metadata["page"],
                                "doc_type": "dao_proposal", # Belge tÃ¼rÃ¼ olarak DAO Ã¶nerisi
                                "chunk_index": i,
                                "proposal_id": proposal_id  # Ã‡Ä±karÄ±lan proposal_id
                            }
                            
                            # LangChain Document formatÄ±nda belgeyi ekler
                            documents.append(Document(
                                page_content=chunk,
                                metadata=metadata
                            ))
                except Exception as e:
                    print(f"PDF iÅŸleme hatasÄ± ({filename}): {str(e)}")
                    traceback.print_exc()
        
        print(f"âœ… {len(documents)} PDF parÃ§asÄ± iÅŸlendi")
        return documents

    def process_csv_data(self, csv_files: List[str]) -> List[Document]:
        """
        Belirtilen CSV dosyalarÄ±nÄ± iÅŸler ve LangChain Document nesneleri listesi dÃ¶ndÃ¼rÃ¼r.
        Dosya adÄ±na gÃ¶re (haber veya tweet) Ã¶zel iÅŸleme yÃ¶ntemlerini Ã§aÄŸÄ±rÄ±r.
        """
        documents = []
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                
                if 'news' in csv_file.lower():
                    docs = self._process_news_csv(df, csv_file)
                elif 'tweet' in csv_file.lower():
                    docs = self._process_tweet_csv(df, csv_file)
                else:
                    print(f"âš ï¸ Bilinmeyen CSV tÃ¼rÃ¼: {csv_file}")
                    continue
                    
                documents.extend(docs) # Ä°ÅŸlenen belgeleri ana listeye ekle
                
            except Exception as e:
                print(f"CSV iÅŸleme hatasÄ± ({csv_file}): {str(e)}")
                traceback.print_exc()
        
        print(f"âœ… {len(documents)} CSV parÃ§asÄ± iÅŸlendi")
        return documents

    def _process_news_csv(self, df: pd.DataFrame, source_file: str) -> List[Document]:
        """Haber CSV verilerini iÅŸler ve Document nesneleri dÃ¶ndÃ¼rÃ¼r."""
        documents = []
        
        for _, row in df.iterrows():
            try:
                # Haber baÅŸlÄ±ÄŸÄ± ve iÃ§eriÄŸini birleÅŸtirir
                content = f"BaÅŸlÄ±k: {row.get('title', '')}\n\n{row.get('content', '')}"
                
                # Ä°Ã§eriÄŸi analiz ederek olay tÃ¼rÃ¼nÃ¼ ve ekonomik etkiyi belirler
                event_type = self.classify_event_type(content)
                economic_impact = self.assess_economic_impact(content, event_type)
                
                # Meta verileri oluÅŸturur
                metadata = {
                    "source": source_file,
                    "doc_type": "news",
                    "title": row.get('title', ''),
                    "url": row.get('url', ''),
                    "date": row.get('date', ''),
                    "event_type": event_type,
                    "economic_significance": economic_impact # Ekonomik Ã¶nem puanÄ±
                }
                
                documents.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
                
            except Exception as e:
                print(f"Haber satÄ±rÄ± iÅŸleme hatasÄ±: {str(e)}")
                continue
        
        return documents

    def _process_tweet_csv(self, df: pd.DataFrame, source_file: str) -> List[Document]:
        """Tweet CSV verilerini iÅŸler ve Document nesneleri dÃ¶ndÃ¼rÃ¼r."""
        documents = []
        
        for _, row in df.iterrows():
            try:
                content = row.get('text', '')
                
                # Tweet'i ekonomik aÃ§Ä±dan analiz eder (token ve NFT koleksiyonlarÄ±)
                economy_analysis = self.analyze_tweet_economy(content)
                # Tweet'in ekonomik etkisini deÄŸerlendirir
                impact_score = self.assess_tweet_impact(content, economy_analysis)
                
                # Meta verileri oluÅŸturur
                metadata = {
                    "source": source_file,
                    "doc_type": "tweet",
                    "tweet_id": row.get('id', ''),
                    "author": row.get('author', ''),
                    "date": row.get('date', ''),
                    "likes": row.get('likes', 0),
                    "retweets": row.get('retweets', 0),
                    "tokens": economy_analysis.get('tokens', []), # Bahsedilen tokenlar
                    "nft_collections": economy_analysis.get('nft_collections', []), # Bahsedilen NFT koleksiyonlarÄ±
                    "economic_significance": impact_score # Ekonomik Ã¶nem puanÄ±
                }
                
                documents.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
                
            except Exception as e:
                print(f"Tweet satÄ±rÄ± iÅŸleme hatasÄ±: {str(e)}")
                continue
        
        return documents

    def classify_event_type(self, content: str) -> str:
        """Ä°Ã§eriÄŸi analiz ederek olayÄ±n tÃ¼rÃ¼nÃ¼ belirler."""
        content_lower = content.lower()
        
        # Anahtar kelimelere dayalÄ± basit sÄ±nÄ±flandÄ±rma
        if any(word in content_lower for word in ['partnership', 'collaboration', 'team up']):
            return 'partnership'
        elif any(word in content_lower for word in ['update', 'upgrade', 'new feature']):
            return 'product_update'
        elif any(word in content_lower for word in ['token', 'airdrop', 'staking', 'aury', 'xaury', 'nerite', 'ember', 'wisdom']):
            return 'tokenomics'
        elif any(word in content_lower for word in ['nft', 'collection', 'mint', 'nefties', 'aurorian', 'aurorians']):
            return 'nft_launch'
        elif any(word in content_lower for word in ['tournament', 'competition', 'event']):
            return 'game_event'
        else:
            return 'general'

    def assess_economic_impact(self, content: str, event_type: str) -> int:
        """Bir iÃ§eriÄŸin ekonomik etkisini 1-5 arasÄ± bir skorla deÄŸerlendirir."""
        content_lower = content.lower()
        score = 1
        
        # Olay tÃ¼rÃ¼ne gÃ¶re temel skor atar
        type_scores = {
            'partnership': 4,
            'tokenomics': 5,
            'nft_launch': 3,
            'product_update': 3,
            'game_event': 2,
            'general': 1
        }
        score = type_scores.get(event_type, 1)
        
        # YÃ¼ksek etki belirten kelimeler iÃ§in bonus puan ekler
        high_impact_words = ['major', 'significant', 'launch', 'million', 'billion', 'funding', 'risk', 'threat', 'vulnerability']
        for word in high_impact_words:
            if word in content_lower:
                score = min(5, score + 1) # Skoru 5'i geÃ§meyecek ÅŸekilde artÄ±r
                # break # Ä°lk eÅŸleÅŸmede dur (isteÄŸe baÄŸlÄ±, daha fazla kelime kontrolÃ¼ iÃ§in kaldÄ±rÄ±labilir)
        
        return score

    def analyze_tweet_economy(self, tweet_text: str) -> Dict[str, Any]:
        """
        Bir tweet'in ekonomik iÃ§eriÄŸini (bahsedilen tokenlar ve NFT koleksiyonlarÄ±) analiz eder.
        NOT: KullanÄ±lan embedding model tweet'lerdeki "Nefties", "Aurorians" gibi
        domain'e Ã¶zel jargon/kÄ±saltmalarÄ± tam olarak tanÄ±mayabilir.
        Bu fonksiyon, bu terimleri anahtar kelimelerle manuel olarak tespit ederek
        arama filtrelerinin daha etkili olmasÄ±nÄ± saÄŸlamaya yardÄ±mcÄ± olur.
        Daha iyi anlama iÃ§in modele fine-tuning yapÄ±lmasÄ± (bu kodun kapsamÄ± dÄ±ÅŸÄ±nda)
        veya daha geliÅŸmiÅŸ bir embedding modeli kullanÄ±lmasÄ± dÃ¼ÅŸÃ¼nÃ¼lebilir.
        """
        tweet_lower = tweet_text.lower()
        
        # Aurory ekosistemine Ã¶zgÃ¼ token isimleri
        token_keywords = ['aury', 'aurory', 'token', 'coin', 'xaury', 'nerite', 'ember', 'wisdom']
        found_tokens = [token for token in token_keywords if token in tweet_lower]
        
        # Aurory NFT koleksiyonlarÄ±
        nft_keywords = ['nefties', 'aurorian', 'aurorians'] 
        found_collections = [nft for nft in nft_keywords if nft in tweet_lower]
        
        return {
            'tokens': list(set(found_tokens)), # Tekrar edenleri kaldÄ±r
            'nft_collections': list(set(found_collections)), # Tekrar edenleri kaldÄ±r
            'has_economic_content': len(found_tokens) > 0 or len(found_collections) > 0
        }

    def assess_tweet_impact(self, tweet_text: str, economy_analysis: Dict[str, Any]) -> int:
        """Bir tweet'in ekonomik etkisini deÄŸerlendirir."""
        tweet_lower = tweet_text.lower()
        score = 1
        
        # Ekonomik iÃ§erik varsa temel skoru artÄ±r
        if economy_analysis.get('has_economic_content', False):
            score = 2
        
        # YÃ¼ksek etki kelimeleri iÃ§in bonus puan
        high_impact_words = ['announcement', 'launch', 'partnership', 'update', 'new', 'major', 'significant', 'price', 'market', 'volume', 'floor', 'risk']
        for word in high_impact_words:
            if word in tweet_lower:
                score = min(5, score + 1)
        
        # Bahsedilen token/NFT sayÄ±sÄ±na gÃ¶re bonus
        total_mentions = len(economy_analysis.get('tokens', [])) + len(economy_analysis.get('nft_collections', []))
        if total_mentions >= 2:
            score = min(5, score + 1)
        
        return score

    def clear_existing_documents(self):
        """Mevcut 'Document' dÃ¼ÄŸÃ¼mlerini ve 'aurory_docs' vektÃ¶r indeksini Neo4j'den temizler."""
        try:
            with self.driver.session() as session:
                # Ã–nce var olan vektÃ¶r indeksini siler
                session.run("DROP INDEX aurory_docs IF EXISTS")
                # TÃ¼m Document dÃ¼ÄŸÃ¼mlerini ve bunlara baÄŸlÄ± tÃ¼m iliÅŸkileri siler
                session.run("MATCH (d:Document) DETACH DELETE d")
                print("âœ… Mevcut dokÃ¼manlar temizlendi")
        except Exception as e:
            print(f"âš ï¸ Temizleme sÄ±rasÄ±nda hata (normal olabilir, eÄŸer indeks veya dÃ¼ÄŸÃ¼m yoksa): {str(e)}")
            traceback.print_exc()

    def store_in_neo4j(self, documents: List[Document], index_name: str = "aurory_docs"):
        """LangChain Document nesnelerini Neo4j'ye kaydeder ve vektÃ¶r indeksine ekler."""
        try:
            # Neo4jVector.from_documents, indeks yoksa otomatik olarak oluÅŸturur
            Neo4jVector.from_documents(
                documents=documents,
                embedding=self.embedding_model,
                url=self.neo4j_uri,
                username=self.neo4j_user,
                password=self.neo4j_password,
                index_name=index_name,              # VektÃ¶r indeksinin adÄ±
                node_label="Document",              # DÃ¼ÄŸÃ¼mlerin etiketi
                text_node_property="content",       # Metin iÃ§eriÄŸi Ã¶zelliÄŸi
                embedding_node_property="embedding" # GÃ¶mÃ¼lÃ¼ vektÃ¶r Ã¶zelliÄŸi
            )
            print(f"âœ… {len(documents)} dokÃ¼man Neo4j'ye kaydedildi")
        except Exception as e:
            print(f"âŒ Neo4j kaydetme hatasÄ±: {str(e)}")
            traceback.print_exc()

    def create_document_relationships(self):
        """
        Ä°ÅŸlenen dokÃ¼manlar ile mevcut Neo4j dÃ¼ÄŸÃ¼mleri (Proposal, Token, Collection) arasÄ±nda
        iliÅŸkiler kurar.
        """
        try:
            with self.driver.session() as session:
                print("ğŸ” Debug: Mevcut Document ve Proposal node'larÄ±nÄ± kontrol ediliyor...")
                
                # DAO Ã¶nerisi dokÃ¼manlarÄ±ndan proposal_id'leri ve kaynak dosyalarÄ± kontrol et
                doc_ids_check = session.run("""
                    MATCH (d:Document {doc_type: 'dao_proposal'})
                    WHERE d.proposal_id IS NOT NULL
                    RETURN d.proposal_id AS proposalId, d.source AS sourceFile
                """)
                found_doc_ids = []
                print("ğŸ“„ Bulunan Document proposal_id'leri:")
                for record in doc_ids_check:
                    found_doc_ids.append(record["proposalId"])
                    print(f"  - Document proposal_id: {record['proposalId']} (from: {record['sourceFile']})")

                # Mevcut Proposal dÃ¼ÄŸÃ¼mlerinin proposalId'lerini kontrol et
                prop_ids_check = session.run("""
                    MATCH (p:Proposal)
                    RETURN p.proposalId AS proposalId
                """)
                found_prop_ids = []
                print("ğŸ›ï¸ Bulunan Proposal proposalId'leri:")
                for record in prop_ids_check:
                    found_prop_ids.append(record["proposalId"])
                    # Tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼ hata ayÄ±klama iÃ§in:
                    print(f"  - Proposal proposalId: {record['proposalId']} (tip: {type(record['proposalId'])})")
                
                # Ortak ID'leri bulup veri tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol et
                set_doc_ids = set(map(str, found_doc_ids))
                set_prop_ids = set(map(str, found_prop_ids))

                common_ids = set_doc_ids.intersection(set_prop_ids)
                if common_ids:
                    print(f"âœ… Document ve Proposal arasÄ±nda ortak ID'ler bulundu: {common_ids}")
                else:
                    print("âŒ Document ve Proposal arasÄ±nda ortak ID bulunamadÄ±. Veri tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol edin. Bu, DAO Ã¶nerileri iÃ§in iliÅŸkilerin kurulmamasÄ±na neden olabilir.")
                
                # 1. DAO Ã¶nerisi dokÃ¼manlarÄ±nÄ± ilgili Proposal dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                # toString() kullanarak tip tutarsÄ±zlÄ±klarÄ±nÄ± Ã¶nler.
                result = session.run("""
                    MATCH (d:Document {doc_type: 'dao_proposal'})
                    WHERE d.proposal_id IS NOT NULL
                    MATCH (p:Proposal)
                    WHERE toString(p.proposalId) = toString(d.proposal_id)
                    MERGE (d)-[:DESCRIBES]->(p)
                    RETURN count(*) as linked_count
                """)
                linked_count = result.single()["linked_count"]
                print(f"âœ… {linked_count} adet Document-Proposal baÄŸlantÄ±sÄ± kuruldu (:DESCRIBES)")
                
                # 2. Haber dokÃ¼manlarÄ±nÄ±, iÃ§eriÄŸinde adÄ± geÃ§en Token dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                # toLower() kullanarak bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf duyarlÄ±lÄ±ÄŸÄ±nÄ± kaldÄ±rÄ±r.
                session.run("""
                    MATCH (d:Document {doc_type: 'news'})
                    MATCH (t:Token)
                    WHERE toLower(d.content) CONTAINS toLower(t.name) 
                    MERGE (d)-[:DISCUSSES]->(t)
                """)
                print("âœ… News-Token baÄŸlantÄ±larÄ± kuruldu (:DISCUSSES)")
                
                # 3. Ekonomik Ã¶nemi yÃ¼ksek haber dokÃ¼manlarÄ±nÄ± ilgili Token dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                session.run("""
                    MATCH (d:Document {doc_type: 'news'})
                    WHERE d.economic_significance >= 3
                    MATCH (t:Token)
                    WHERE toLower(d.content) CONTAINS toLower(t.name)
                    MERGE (d)-[:AFFECTS]->(t)
                """)
                print("âœ… News-Token baÄŸlantÄ±larÄ± kuruldu (:AFFECTS) [Ekonomik Ã–nem > 3]")
                
                # 4. Tweet dokÃ¼manlarÄ±nÄ±, meta verilerinde bahsedilen Token dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                session.run("""
                    MATCH (d:Document {doc_type: 'tweet'})
                    WHERE d.tokens IS NOT NULL
                    UNWIND d.tokens AS tokenName
                    MATCH (t:Token {name: toLower(tokenName)})
                    MERGE (d)-[:REFERENCES]->(t)
                """)
                print("âœ… Tweet-Token baÄŸlantÄ±larÄ± kuruldu (:REFERENCES)")
                
                # 5. Tweet dokÃ¼manlarÄ±nÄ±, meta verilerinde bahsedilen NFT Collection dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                # Cypher sorgusu iÃ§indeki yorum kaldÄ±rÄ±ldÄ±.
                session.run("""
                    MATCH (d:Document {doc_type: 'tweet'})
                    WHERE d.nft_collections IS NOT NULL
                    UNWIND d.nft_collections AS collectionName
                    MATCH (c:Collection {name: collectionName}) 
                    MERGE (d)-[:ABOUT]->(c)
                """)
                print("âœ… Tweet-Collection baÄŸlantÄ±larÄ± kuruldu (:ABOUT)")
                
                # 6. Ekonomik Ã¶nemi yÃ¼ksek tweet dokÃ¼manlarÄ±nÄ± ilgili Token dÃ¼ÄŸÃ¼mlerine baÄŸlar.
                session.run("""
                    MATCH (d:Document {doc_type: 'tweet'})
                    WHERE d.economic_significance >= 3
                    MATCH (t:Token)
                    WHERE toLower(d.content) CONTAINS toLower(t.name)
                    MERGE (d)-[:POTENTIAL_IMPACT]->(t)
                """)
                print("âœ… Tweet-Token baÄŸlantÄ±larÄ± kuruldu (:POTENTIAL_IMPACT) [Ekonomik Ã–nem > 3]")
                
                print("âœ… TÃ¼m iliÅŸkiler baÅŸarÄ±yla oluÅŸturuldu")
        except Exception as e:
            print(f"âŒ Ä°liÅŸki kurma hatasÄ±: {str(e)}")
            traceback.print_exc()

    def semantic_search(self, query_text: str, limit: int = 5, filters: Dict[str, Any] = None, score_threshold: float = 0.65) -> List[Document]:
        """
        Belirtilen sorgu metnine gÃ¶re anlamsal arama yapar ve isteÄŸe baÄŸlÄ± filtreler uygular.
        score_threshold: DÃ¶ndÃ¼rÃ¼lecek dokÃ¼manlarÄ±n sahip olmasÄ± gereken minimum benzerlik puanÄ±.
        """
        try:
            # Temel retrieval sorgusu: TÃ¼m Document Ã¶zelliklerini metadata olarak dÃ¶ndÃ¼rÃ¼r
            # Cypher'da RETURN iÃ§inde # yorumlarÄ± kullanÄ±lamaz, kaldÄ±rÄ±ldÄ±.
            base_retrieval_query = """
            RETURN node.content AS text, score, {
                source: node.source,
                page: node.page,
                doc_type: node.doc_type,
                chunk_index: node.chunk_index,
                proposal_id: node.proposal_id,
                title: node.title,
                url: node.url,
                date: node.date,
                event_type: node.event_type,
                economic_significance: node.economic_significance,
                tweet_id: node.tweet_id,
                author: node.author,
                likes: node.likes,
                retweets: node.retweets,
                tokens: node.tokens, 
                nft_collections: node.nft_collections
            } AS metadata
            """

            where_clauses = []
            if filters:
                # Belge tÃ¼rÃ¼ne gÃ¶re filtreleme
                if "doc_type" in filters and filters["doc_type"] is not None:
                    where_clauses.append(f"node.doc_type = '{filters['doc_type']}'")
                # Minimum ekonomik Ã¶nem puanÄ±na gÃ¶re filtreleme
                if "economic_significance_min" in filters and filters["economic_significance_min"] is not None:
                    where_clauses.append(f"node.economic_significance >= {filters['economic_significance_min']}")
                # Proposal ID'ye gÃ¶re filtreleme (DAO Ã¶nerileri iÃ§in)
                if "proposal_id" in filters and filters["proposal_id"] is not None:
                    where_clauses.append(f"toString(node.proposal_id) = '{filters['proposal_id']}'")
                # Yazar adÄ±na gÃ¶re filtreleme (tweetler iÃ§in)
                if "author" in filters and filters["author"] is not None:
                    where_clauses.append(f"toLower(node.author) CONTAINS toLower('{filters['author']}')")
                # Bahsedilen tokenlara gÃ¶re filtreleme
                if "tokens_mentioned" in filters and filters["tokens_mentioned"] is not None:
                    # `tokens` Ã¶zelliÄŸi bir string listesi olduÄŸu varsayÄ±mÄ±yla
                    token_list_str = ", ".join([f"'{token.lower()}'" for token in filters['tokens_mentioned']])
                    where_clauses.append(f"ANY(t IN node.tokens WHERE toLower(t) IN [{token_list_str}])")
                # Bahsedilen NFT koleksiyonlarÄ±na gÃ¶re filtreleme
                if "nft_collections_mentioned" in filters and filters["nft_collections_mentioned"] is not None:
                    collection_list_str = ", ".join([f"'{col.lower()}'" for col in filters['nft_collections_mentioned']])
                    where_clauses.append(f"ANY(c IN node.nft_collections WHERE toLower(c) IN [{collection_list_str}])")
            
            # Dinamik Cypher sorgusunu oluÅŸtur
            final_retrieval_query = "MATCH (node:Document)"
            if where_clauses:
                final_retrieval_query += " WHERE " + " AND ".join(where_clauses)
            final_retrieval_query += base_retrieval_query

            # FiltrelenmiÅŸ arama iÃ§in yeni Neo4jVector nesnesi oluÅŸtur
            # Benzerlik aramasÄ± iÃ§in daha fazla aday almak adÄ±na k limitini iki katÄ±na Ã§Ä±karÄ±yoruz,
            # bÃ¶ylece eÅŸik sonrasÄ± filtreleme iÃ§in yeterli veri olur.
            vector_store = Neo4jVector.from_existing_index(
                embedding=self.embedding_model,
                url=self.neo4j_uri,
                username=self.neo4j_user,
                password=self.neo4j_password,
                index_name="aurory_docs",
                text_node_property="content",
                embedding_node_property="embedding", # Embedding Ã¶zelliÄŸi burada da belirtilmeli
                retrieval_query=final_retrieval_query # Dinamik sorguyu buraya iletiyoruz
            )

            # Benzerlik aramasÄ± yapar ve sonuÃ§larÄ± dÃ¶ndÃ¼rÃ¼r. Score da dahil olmak Ã¼zere.
            # Daha sonra score_threshold ile filtrelenecek.
            docs_and_scores = vector_store.similarity_search_with_score(query_text, k=limit*2) # Limit*2 ile daha fazla aday al

            # EÅŸiÄŸe gÃ¶re filtreleme
            filtered_results = [
                doc for doc, score in docs_and_scores if score >= score_threshold
            ]
            
            # Ä°stenen limit kadar sonuÃ§ dÃ¶ndÃ¼r
            return filtered_results[:limit]
        except Exception as e:
            print(f"âŒ Semantic search hatasÄ±: {str(e)}")
            traceback.print_exc()
            return []

    def get_nft_market_buzz(self, nft_collections: List[str], limit: int = 5) -> Dict[str, List[Document]]:
        """
        "How are Nefties and Aurorians performing in the market, and what's the Twitter buzz around these collections?"
        Bu kullanÄ±m durumu iÃ§in NFT koleksiyonlarÄ±nÄ±n piyasa performansÄ±nÄ± ve Twitter tartÄ±ÅŸmalarÄ±nÄ± getirir.
        """
        results = {}
        for collection_name in nft_collections:
            print(f"\n--- '{collection_name}' iÃ§in Arama BaÅŸlatÄ±lÄ±yor ---")
            # NFT koleksiyonu ile ilgili tweet'leri arar
            tweet_query = f"Latest discussions, sentiment, and market buzz around {collection_name} on Twitter."
            # Filtreye koleksiyon adÄ±nÄ±n kÃ¼Ã§Ã¼k harf karÅŸÄ±lÄ±ÄŸÄ±nÄ± ekleriz
            tweet_filters = {"doc_type": "tweet", "nft_collections_mentioned": [collection_name.lower()]}
            # VarsayÄ±lan score_threshold (0.65) burada kullanÄ±lacak. Ä°sterseniz deÄŸiÅŸtirebilirsiniz.
            tweets = self.semantic_search(tweet_query, limit=limit, filters=tweet_filters)
            results[f"{collection_name}_tweets"] = tweets
            print(f"âœ… {len(tweets)} adet '{collection_name}' ile ilgili tweet bulundu.")

            # NFT koleksiyonu ile ilgili ekonomik haberleri arar
            news_query = f"Market performance, floor price trends, and news about {collection_name}."
            # Ekonomik Ã¶nemi olan haberleri filtreleriz
            news_filters = {"doc_type": "news", "economic_significance_min": 2} 
            # VarsayÄ±lan score_threshold (0.65) burada kullanÄ±lacak. Ä°sterseniz deÄŸiÅŸtirebilirsiniz.
            news = self.semantic_search(news_query, limit=limit, filters=news_filters)
            results[f"{collection_name}_news"] = news
            print(f"âœ… {len(news)} adet '{collection_name}' ile ilgili haber/ekonomik veri bulundu.")
        return results

    def get_dao_community_response(self, query: str = "active DAO proposals and community response", limit: int = 5) -> Dict[str, List[Document]]:
        """
        "What are the active DAO proposals and how is the community responding to them on social media?"
        Bu kullanÄ±m durumu iÃ§in aktif DAO Ã¶nerilerini ve topluluk tepkilerini getirir.
        """
        results = {}
        print(f"\n--- '{query}' iÃ§in Arama BaÅŸlatÄ±lÄ±yor ---")
        
        # DAO Ã¶nerileri iÃ§in arama yapar
        dao_filters = {"doc_type": "dao_proposal"}
        # 'active' durumunu doÄŸrudan Document'Ä±n meta verisinde tutmadÄ±ÄŸÄ±mÄ±z iÃ§in, arama metni ile hedefleriz
        dao_proposals = self.semantic_search("What are the current active DAO proposals?", limit=limit, filters=dao_filters)
        results["dao_proposals"] = dao_proposals
        print(f"âœ… {len(dao_proposals)} adet DAO Ã¶nerisi bulundu.")

        # Topluluk tepkileri iÃ§in tweet'leri arar
        # DAO Ã¶nerileriyle ilgili tweet'lerin `proposal_id`'si tweet meta verisinde yok,
        # bu yÃ¼zden genel DAO tartÄ±ÅŸmalarÄ±nÄ± ararÄ±z veya daha sonra manuel olarak iliÅŸkilendirebiliriz.
        community_tweet_query = f"Community sentiment, discussions, and response regarding DAO proposals."
        community_tweets = self.semantic_search(community_tweet_query, limit=limit, filters={"doc_type": "tweet", "economic_significance_min": 1})
        results["community_tweets"] = community_tweets
        print(f"âœ… {len(community_tweets)} adet topluluk tepkisi tweet'i bulundu.")
        return results

    def get_token_market_sentiment(self, token_name: str, limit: int = 5) -> Dict[str, List[Document]]:
        """
        "What's the current market sentiment for AURY tokens based on recent Twitter discussions and price movements?"
        Bu kullanÄ±m durumu iÃ§in belirli bir token iÃ§in piyasa duyarlÄ±lÄ±ÄŸÄ±nÄ± ve Twitter tartÄ±ÅŸmalarÄ±nÄ± getirir.
        """
        results = {}
        print(f"\n--- '{token_name}' iÃ§in Piyasa DuyarlÄ±lÄ±ÄŸÄ± ve Twitter TartÄ±ÅŸmasÄ± Arama BaÅŸlatÄ±lÄ±yor ---")
        
        # Token ile ilgili tweet'leri arar
        tweet_query = f"Market sentiment, discussions, and price buzz about {token_name} token on Twitter."
        # token_name'i kÃ¼Ã§Ã¼k harfe Ã§evirerek filtrelere ekleriz
        tweet_filters = {"doc_type": "tweet", "tokens_mentioned": [token_name.lower()], "economic_significance_min": 2}
        tweets = self.semantic_search(tweet_query, limit=limit, filters=tweet_filters)
        results["token_tweets"] = tweets
        print(f"âœ… {len(tweets)} adet '{token_name}' ile ilgili tweet bulundu.")

        # Token ile ilgili haberleri (fiyat hareketleri iÃ§in) arar
        news_query = f"Price movements, market trends, and economic news about {token_name} token."
        news_filters = {"doc_type": "news", "economic_significance_min": 3, "tokens_mentioned": [token_name.lower()]}
        news = self.semantic_search(news_query, limit=limit, filters=news_filters)
        results["token_news"] = news
        print(f"âœ… {len(news)} adet '{token_name}' ile ilgili haber bulundu.")
        return results

    def get_player_earning_strategies(self, query: str = "successful player earning strategies", limit: int = 5) -> Dict[str, List[Document]]:
        """
        "What earning strategies are successful players discussing on Twitter, and what does the current token economics suggest?"
        Bu kullanÄ±m durumu iÃ§in baÅŸarÄ±lÄ± oyuncu kazanÃ§ stratejilerini ve token ekonomisi iliÅŸkisini getirir.
        """
        results = {}
        print(f"\n--- '{query}' iÃ§in Arama BaÅŸlatÄ±lÄ±yor ---")
        
        # Oyuncu stratejileri ile ilgili tweet'leri arar
        tweet_filters = {"doc_type": "tweet"}
        player_tweets = self.semantic_search("Player earning strategies, tips, gameplay tactics, and discussions in Aurory.", limit=limit, filters=tweet_filters)
        results["player_strategy_tweets"] = player_tweets
        print(f"âœ… {len(player_tweets)} adet oyuncu stratejisi tweet'i bulundu.")

        # Token ekonomisi ile ilgili genel dokÃ¼manlarÄ± (haberler) arar
        economy_news_filters = {"doc_type": "news", "economic_significance_min": 3}
        economy_news = self.semantic_search("Aurory token economics, staking mechanisms, rewards, inflation, and deflation.", limit=limit, filters=economy_news_filters)
        results["token_economy_news"] = economy_news
        print(f"âœ… {len(economy_news)} adet token ekonomisi haberi bulundu.")
        return results

    def get_ecosystem_economic_risks(self, query: str = "main economic risks for Aurory ecosystem", limit: int = 5) -> Dict[str, List[Document]]:
        """
        "What are the main economic risks for Aurory ecosystem based on recent data and community discussions?"
        Bu kullanÄ±m durumu iÃ§in Aurory ekosistemi iÃ§in ana ekonomik riskleri getirir.
        """
        results = {}
        print(f"\n--- '{query}' iÃ§in Arama BaÅŸlatÄ±lÄ±yor ---")
        
        # Ekonomik riskleri belirten haberleri arar (yÃ¼ksek ekonomik Ã¶nemde olanlar)
        news_filters = {"doc_type": "news", "economic_significance_min": 4} # Risks usually have higher significance
        risk_news = self.semantic_search("Potential economic threats, vulnerabilities, market risks, and financial concerns in Aurory.", limit=limit, filters=news_filters)
        results["risk_news"] = risk_news
        print(f"âœ… {len(risk_news)} adet ekonomik risk haberi bulundu.")

        # Topluluk tartÄ±ÅŸmalarÄ±nda geÃ§en riskleri arar (tweetler)
        tweet_filters = {"doc_type": "tweet", "economic_significance_min": 3}
        risk_tweets = self.semantic_search("Community concerns, potential issues, and economic risks discussions on social media.", limit=limit, filters=tweet_filters)
        results["risk_tweets"] = risk_tweets
        print(f"âœ… {len(risk_tweets)} adet risk odaklÄ± tweet bulundu.")
        return results


    def close_connection(self):
        """Neo4j baÄŸlantÄ±sÄ±nÄ± kapatÄ±r."""
        if self.driver:
            self.driver.close()
            print("âœ… Neo4j baÄŸlantÄ±sÄ± kapatÄ±ldÄ±")

def print_search_results(title: str, results: List[Document]):
    """Arama sonuÃ§larÄ±nÄ± baÅŸlÄ±k ve detaylarla dÃ¼zenli bir ÅŸekilde konsola yazdÄ±rÄ±r."""
    print(f"\n--- {title} ---")
    if not results:
        print("SonuÃ§ bulunamadÄ±.")
        return
    for i, doc in enumerate(results):
        source = doc.metadata.get('source', 'Bilinmiyor')
        doc_type = doc.metadata.get('doc_type', 'Bilinmiyor')
        # Daha bilgilendirici bir baÅŸlÄ±k veya ID seÃ§imi
        content_id = doc.metadata.get('title', doc.metadata.get('tweet_id', doc.metadata.get('proposal_id', 'BaÅŸlÄ±k Yok')))
        impact = doc.metadata.get('economic_significance', 0)
        # Ä°Ã§eriÄŸi kÄ±salt ve sonunda '...' ekle
        content = doc.page_content[:250] + "..." if len(doc.page_content) > 250 else doc.page_content
        
        print(f"\nğŸ” SONUÃ‡ {i+1} - TÃ¼r: {doc_type.upper()} - Etki: {'â­' * impact}")
        print(f"ğŸ“° ID/BaÅŸlÄ±k: {content_id}")
        print(f"ğŸ“ Kaynak: {source}")
        print("-" * 50)
        print(content)
        print("-" * 50)

def main():
    """
    Ana fonksiyon, pipeline'Ä± baÅŸlatÄ±r, verileri iÅŸler, Neo4j'ye kaydeder,
    iliÅŸkileri kurar ve tanÄ±mlanan kullanÄ±m senaryolarÄ± iÃ§in test aramalarÄ± yapar.
    """
    pipeline = VectorEmbeddingPipeline()
    
    try:
        # Neo4j baÄŸlantÄ±sÄ±nÄ± test et
        print("\n" + "="*50)
        print("Neo4j BaÄŸlantÄ± Testi...")
        print("="*50)
        if not pipeline.test_connection():
            print("âŒ Neo4j baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z. Ä°ÅŸlemler durduruluyor.")
            return
            
        # PDF ve CSV dosyalarÄ±nÄ±n yollarÄ±nÄ± tanÄ±mlayÄ±n.
        # Bu yollarÄ±n kendi sisteminizdeki gerÃ§ek yollarla eÅŸleÅŸtiÄŸinden emin olun.
        pdf_folder = "C:/Users/alice/OneDrive/MasaÃ¼stÃ¼/P2E-Economy-Assistant-AuroryGame/data_collection/DAOPDF"
        csv_files = [
            "C:/Users/alice/OneDrive/MasaÃ¼stÃ¼/P2E-Economy-Assistant-AuroryGame/embedding_data/aurory_tweets.csv",
            "C:/Users/alice/OneDrive/MasaÃ¼stÃ¼/P2E-Economy-Assistant-AuroryGame/embedding_data/aurory_news.csv"
        ]
        
        # 1. PDF dokÃ¼manlarÄ±nÄ± iÅŸle (DAO Ã¶nerileri)
        print("\n" + "="*50)
        print("PDF DosyalarÄ± Ä°ÅŸleniyor...")
        print("="*50)
        pdf_docs = pipeline.process_pdf_documents(pdf_folder)
        
        # 2. CSV verilerini iÅŸle (haberler ve tweetler)
        print("\n" + "="*50)
        print("CSV DosyalarÄ± Ä°ÅŸleniyor...")
        print("="*50)
        csv_docs = pipeline.process_csv_data(csv_files)
        
        # 3. TÃ¼m iÅŸlenmiÅŸ dokÃ¼manlarÄ± birleÅŸtir
        all_documents = pdf_docs + csv_docs
        print(f"\nToplam {len(all_documents)} dokÃ¼man hazÄ±r")
        
        # 3.5. Mevcut dokÃ¼manlarÄ± ve indeksi temizle (her Ã§alÄ±ÅŸtÄ±rmada temiz bir baÅŸlangÄ±Ã§ iÃ§in)
        print("\n" + "="*50)
        print("Mevcut DokÃ¼manlar Temizleniyor...")
        print("="*50)
        pipeline.clear_existing_documents()
        
        # 4. Ä°ÅŸlenen dokÃ¼manlarÄ± Neo4j'ye kaydet ve embedding'lerini oluÅŸtur
        print("\n" + "="*50)
        print("Neo4j'ye Kaydediliyor...")
        print("="*50)
        pipeline.store_in_neo4j(all_documents)
        
        # 5. DokÃ¼manlar ve diÄŸer varlÄ±klar arasÄ±nda iliÅŸkileri kur (Token, Collection, Proposal)
        print("\n" + "="*50)
        print("Graf Ä°liÅŸkileri Kuruluyor...")
        print("="*50)
        pipeline.create_document_relationships()
        
        # 6. TanÄ±mlanan kullanÄ±m senaryolarÄ± iÃ§in test aramalarÄ± yap
        print("\n" + "="*50)
        print("KullanÄ±m Ã–rnekleri Ä°Ã§in Test AramalarÄ± BaÅŸlatÄ±lÄ±yor...")
        print("="*50)

        # Use Case 1: Nefties ve Aurorians piyasa performansÄ± ve Twitter buzz
        print("\n>>> KULLANIM SENARYOSU 1: Nefties ve Aurorians'Ä±n piyasa performansÄ± ve Twitter'daki yankÄ±sÄ± <<<")
        # score_threshold'u Ã¶rnek olarak 0.60'a dÃ¼ÅŸÃ¼rerek daha fazla eÅŸleÅŸme yakalamayÄ± deneyelim
        nft_buzz_results = pipeline.get_nft_market_buzz(["Nefties", "Aurorians"], limit=2)
        print_search_results("Nefties Twitter Buzz (Ã–rnek Tweetler)", nft_buzz_results.get("Nefties_tweets", []))
        print_search_results("Nefties Market News (Ã–rnek Haberler)", nft_buzz_results.get("Nefties_news", []))
        print_search_results("Aurorians Twitter Buzz (Ã–rnek Tweetler)", nft_buzz_results.get("Aurorians_tweets", []))
        print_search_results("Aurorians Market News (Ã–rnek Haberler)", nft_buzz_results.get("Aurorians_news", []))
        
        # Use Case 2: Aktif DAO Ã¶nerileri ve topluluk tepkisi
        print("\n>>> KULLANIM SENARYOSU 2: Aktif DAO Ã¶nerileri ve topluluÄŸun sosyal medyadaki tepkisi <<<")
        dao_community_results = pipeline.get_dao_community_response(limit=3)
        print_search_results("Aktif DAO Ã–nerileri (PDF DokÃ¼manlarÄ±)", dao_community_results["dao_proposals"])
        print_search_results("DAO Topluluk Tepkileri (Genel Tweet TartÄ±ÅŸmalarÄ±)", dao_community_results["community_tweets"])

        # Use Case 3: AURY token piyasa duyarlÄ±lÄ±ÄŸÄ± ve fiyat hareketleri
        print("\n>>> KULLANIM SENARYOSU 3: AURY tokenleri iÃ§in mevcut piyasa duyarlÄ±lÄ±ÄŸÄ± ve fiyat hareketleri <<<")
        # score_threshold'u Ã¶rnek olarak 0.55'e dÃ¼ÅŸÃ¼rerek daha fazla eÅŸleÅŸme yakalamayÄ± deneyelim
        aury_sentiment_results = pipeline.get_token_market_sentiment("AURY", limit=3)
        print_search_results("AURY Token Twitter DuyarlÄ±lÄ±ÄŸÄ±", aury_sentiment_results["token_tweets"])
        print_search_results("AURY Token Piyasa Haberleri", aury_sentiment_results["token_news"])

        # Use Case 4: Oyuncu kazanÃ§ stratejileri ve token ekonomisi
        print("\n>>> KULLANIM SENARYOSU 4: BaÅŸarÄ±lÄ± oyuncularÄ±n Twitter'da tartÄ±ÅŸtÄ±ÄŸÄ± kazanÃ§ stratejileri ve mevcut token ekonomisi <<<")
        player_strategy_results = pipeline.get_player_earning_strategies(limit=3)
        print_search_results("Oyuncu KazanÃ§ Stratejileri (Tweetler)", player_strategy_results["player_strategy_tweets"])
        print_search_results("Token Ekonomisi Bilgisi (Haberler)", player_strategy_results["token_economy_news"])

        # Use Case 5: Aurory ekosistemi ana ekonomik riskleri
        print("\n>>> KULLANIM SENARYUSU 5: Aurory ekosistemi iÃ§in ana ekonomik riskler <<<")
        ecosystem_risk_results = pipeline.get_ecosystem_economic_risks(limit=3)
        print_search_results("Ekonomik Risk Haberleri", ecosystem_risk_results["risk_news"])
        print_search_results("Topluluk Risk TartÄ±ÅŸmalarÄ± (Tweetler)", ecosystem_risk_results["risk_tweets"])

    finally:
        # Program sonunda Neo4j baÄŸlantÄ±sÄ±nÄ± kapat
        pipeline.close_connection()

if __name__ == "__main__":
    main()
